{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f1a3fd0-b689-4d96-b2da-aafef25f9afb",
   "metadata": {},
   "source": [
    "# Satellite-based Property Price Prediction\n",
    "\n",
    "## Contents\n",
    "1. Problem Statement\n",
    "2. Dataset Description\n",
    "3. Exploratory Data Analysis (EDA)\n",
    "4. Feature Engineering & Preprocessing\n",
    "5. Train–Test Split\n",
    "6. Baseline Model (Tabular Only)\n",
    "7. XGBoost Model (Tabular Only)\n",
    "8. Multimodal Model (Tabular + Satellite Images)\n",
    "9. Model Evaluation & Comparison\n",
    "10. Final Predictions\n",
    "11. Conclusion & Future Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e21bc52-9f4f-417b-965f-638df7662acd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eacf715-0e96-4589-96c1-30e5358d50f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cab9f048-6980-426a-b6b3-6ec09557f0c3",
   "metadata": {},
   "source": [
    "# SECTION 1 — Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe9394b-c5ad-48a8-bef9-42d8f235ed8d",
   "metadata": {},
   "source": [
    "# Satellite-Based Property Price Prediction\n",
    "\n",
    "## Problem Statement\n",
    "Accurately estimating property prices is critical for real estate valuation,\n",
    "investment analysis, and urban planning. Traditional models rely mainly on\n",
    "tabular property attributes such as location, size, and amenities.\n",
    "\n",
    "In this project, we aim to improve property price prediction by combining:\n",
    "- **Tabular property data** (numerical and categorical features)\n",
    "- **Satellite imagery** capturing neighborhood-level visual characteristics\n",
    "\n",
    "The objective is to compare the performance of:\n",
    "1. A tabular-data-only model\n",
    "2. A multimodal model that integrates tabular data with satellite images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5683932f-d541-4cdc-b204-681f33e14869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52e9658-a8d7-4ce5-8f4e-32379738ff1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e874df2f-a3c2-4434-8143-9e97c98ef205",
   "metadata": {},
   "source": [
    "# SECTION 2 — Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b3118c-4a67-4ec9-8ef2-0e53e28b8dfc",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "The dataset consists of two main components:\n",
    "\n",
    "### 1. Tabular Property Data\n",
    "The tabular dataset contains structured information about properties, such as:\n",
    "- Property identifiers\n",
    "- Location-related attributes\n",
    "- Structural characteristics\n",
    "- Other numerical and categorical features relevant to price prediction\n",
    "\n",
    "The target variable is:\n",
    "- **Property Price**, which represents the market value of each property.\n",
    "\n",
    "### 2. Satellite Images\n",
    "For each property, a satellite image is available that captures the surrounding\n",
    "environment (e.g., greenery, road density, building patterns). These images are\n",
    "used to extract visual features that may influence property prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edd4e72-3822-4b37-bdde-c03840d14ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d43ddef-efe9-4cde-ae90-0ee384c33b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Imports & Configuration\n",
    "# ========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "DATA_PATH = \"./\"          # current folder\n",
    "IMAGE_PATH = \"./images/\"  # images folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319a70b0-02e3-459c-b147-0e5f4b8eba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data (Excel file)\n",
    "df = pd.read_excel(DATA_PATH + \"train(1).xlsx\")\n",
    "\n",
    "# Basic dataset overview\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49670cf9-c37f-474d-a044-9c1c37b0c502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f3f0a6-f2f5-4d8a-b1ab-de765da0c1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Images found:\", os.listdir(IMAGE_PATH)[:5])\n",
    "print(\"Data shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd13b9b-6332-4547-8d7f-fe4b7a4c1100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e01d58dc-525b-4568-ba23-224cfe50e4ef",
   "metadata": {},
   "source": [
    "### Satellite Image Acquisition (Programmatic)\n",
    "\n",
    "Satellite images were programmatically acquired using the Mapbox Static Images API.\n",
    "Each image is centered on the property location using latitude and longitude\n",
    "coordinates and captures the surrounding environment, including buildings,\n",
    "roads, greenery, and water bodies.\n",
    "\n",
    "To ensure reproducibility and avoid redundant API calls, the download logic\n",
    "checks for existing images and skips re-downloading when images are already present.\n",
    "\n",
    "# we are using Mapbox Static API for satellite images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ee9f27-7513-4084-b45c-863e7954f58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each house, use: Latitude and Longitude to download a satellite image showing: Buildings, Roads, Green areas, Water\n",
    "# 1. Get a Mapbox Access Token\n",
    "# Default Public Token = \"YOUR_MAPBOX_TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d81e56-9e71-4721-8402-f181d08e0eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create Folder to Save Images\n",
    "import os\n",
    "\n",
    "os.makedirs(IMAGE_PATH, exist_ok=True)\n",
    "\n",
    "if len(os.listdir(IMAGE_PATH)) == 0:\n",
    "    print(\"Downloading satellite images...\")\n",
    "    # ⬇️ MOVE ALL YOUR DOWNLOAD CODE INSIDE HERE ⬇️\n",
    "else:\n",
    "    print(\"Satellite images already exist. Skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a8d52a-1c5b-4198-b04a-64a42726bbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Decide Image Settings (Keep Fixed)\n",
    "# Zoom                     18                             Property + neighborhood\n",
    "# Size                     224 cross 224                        CNN standard\n",
    "# Style                    satellite-v9                   Satellite imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295987ef-f9df-4918-84a3-0f1019312d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Download ONE Test Satellite Image\n",
    "import requests\n",
    "\n",
    "lat = df.loc[0, \"lat\"]\n",
    "lon = df.loc[0, \"long\"]\n",
    "\n",
    "MAPBOX_TOKEN = \"YOUR_MAPBOX_TOKEN\"\n",
    "\n",
    "url = (\n",
    "    f\"https://api.mapbox.com/styles/v1/mapbox/satellite-v9/static/\"\n",
    "    f\"{lon},{lat},18/512x512\"\n",
    "    f\"?access_token={MAPBOX_TOKEN}\"\n",
    ")\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "print(\"Status:\", response.status_code)\n",
    "print(\"Type:\", response.headers.get(\"Content-Type\"))\n",
    "\n",
    "if response.headers.get(\"Content-Type\", \"\").startswith(\"image\"):\n",
    "    with open(\"images/test_image.png\", \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(\"✅ Image saved successfully\")\n",
    "else:\n",
    "    print(\"❌ Error from Mapbox:\")\n",
    "    print(response.text)\n",
    "\n",
    "\n",
    "# --- IMAGE DOWNLOAD (already completed) ---\n",
    "# (Skipped to avoid re-downloading images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba6ed3c-b0a2-468c-b619-f0b4ba1e566c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view test image\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = Image.open(\"images/test_image.png\")\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cb1b1f-da0e-4ab3-9923-f5083831f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.Bulk Image Download Code for 500 images\n",
    "\n",
    "# Create a random sample\n",
    "df_sample = df.sample(n=500, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73f24f8-704d-40bf-be4e-4b001944b213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IMAGE DOWNLOAD (already completed) ---\n",
    "# (Skipped to avoid re-downloading images)\n",
    "\n",
    "# Download satellite images for the random sample\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "MAPBOX_TOKEN = \"YOUR_MAPBOX_TOKEN\"\n",
    "\n",
    "for i, row in enumerate(tqdm(df_sample.iterrows(), total=500)):\n",
    "    lat = row[1][\"lat\"]\n",
    "    lon = row[1][\"long\"]\n",
    "\n",
    "    url = (\n",
    "        f\"https://api.mapbox.com/styles/v1/mapbox/satellite-v9/static/\"\n",
    "        f\"{lon},{lat},18/224x224\"\n",
    "        f\"?access_token={MAPBOX_TOKEN}\"\n",
    "    )\n",
    "\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.headers.get(\"Content-Type\", \"\").startswith(\"image\"):\n",
    "        with open(f\"images/{i}.png\", \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "print(\"✅ Random 500 satellite images downloaded successfully!\")\n",
    "\n",
    "# --- IMAGE DOWNLOAD (already completed) ---\n",
    "# (Skipped to avoid re-downloading images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b08efd4-906a-41ba-8402-5a04b33d2dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all images are created or not\n",
    "import os\n",
    "\n",
    "print(len(os.listdir(\"images\")))\n",
    "print(os.listdir(\"images\")[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c65adee-19e2-48c3-b20c-80ba2d58c3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = Image.open(\"images/1.png\")\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5840a3dd-bc85-4449-b4d0-e631f8ac2bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7ce21a-ba42-421d-8bf8-a42b3bfe566b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a44bab-9a60-456f-ba92-13db89b6ed53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea0b8ec1-92a2-4ac5-913e-ed95b921bb4a",
   "metadata": {},
   "source": [
    "**SECTION 3 — Exploratory Data Analysis (EDA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debd85f6-017b-4c6b-bc1a-314b9a109a1f",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "This section explores the distribution of the target variable,\n",
    "identifies missing values, and visualizes sample satellite images\n",
    "to gain intuition about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced6e06d-8f42-4c58-b2bf-3379ca15bc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060d8bdd-b0fd-46ad-b43e-14e6716c07b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values per column\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values[missing_values > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcde140-c012-46c7-bdfe-b3ed8ac6223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price Distribution\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181f6466-06e2-4bdf-a454-d4a90d6eb7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(df[\"price\"], bins=50)\n",
    "plt.title(\"House Price Distribution\")\n",
    "plt.xlabel(\"Price\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0715b8b7-0c7f-43e2-96fc-633477210514",
   "metadata": {},
   "source": [
    "# Observation:\n",
    "House prices are right-skewed, with many lower-priced homes and a long tail of high-value properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6351f85-5f4d-40a7-ae93-39e29615ce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log-Transformed Price Distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(np.log1p(df[\"price\"]), bins=50)\n",
    "plt.title(\"Log-Transformed House Price Distribution\")\n",
    "plt.xlabel(\"log(price)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ec3e0f-9a52-470e-82f8-bfd73f571755",
   "metadata": {},
   "source": [
    "# Observation:\n",
    "The log transformation produces a more symmetric distribution, which is beneficial for regression modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba671af-f362-45a2-8fa4-c9c7b58bb1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geospatial Distribution of Prices\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(\n",
    "    df[\"long\"],\n",
    "    df[\"lat\"],\n",
    "    c=df[\"price\"],\n",
    "    cmap=\"viridis\",\n",
    "    s=5\n",
    ")\n",
    "plt.colorbar(label=\"Price\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.title(\"House Prices by Location\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc5a548-c6e5-427f-986a-8faa62da9e89",
   "metadata": {},
   "source": [
    "# Observation:\n",
    "Distinct high-price clusters appear in waterfront and dense urban regions, indicating strong spatial effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fc551b-465f-4227-a59f-b8e251c85f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waterfront Effect on Price\n",
    "df.groupby(\"waterfront\")[\"price\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5653c3-4f6b-4ab9-9cb2-0d7cabf2fbee",
   "metadata": {},
   "source": [
    "# Observation:\n",
    "Properties with waterfront access have significantly higher average prices, validating the importance of environmental context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2b4b3b-4dc0-49f7-b7e0-95084ef13805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Quality vs Price\n",
    "df.groupby(\"view\")[\"price\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2835f4-e4f3-439c-874e-e7a78f0df7c7",
   "metadata": {},
   "source": [
    "# Observation:\n",
    "Better views are associated with higher property prices, supporting the use of satellite imagery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d3a5f0-5f94-471a-bc3d-551f93b092c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Living Area vs Price\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(df[\"sqft_living\"], df[\"price\"], alpha=0.3)\n",
    "plt.xlabel(\"Sqft Living Area\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"Living Area vs Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739fd04e-b38c-4902-9733-516bd6390150",
   "metadata": {},
   "source": [
    "# Observation:\n",
    "Larger living areas generally correspond to higher prices, with some dispersion due to location and amenities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add63531-08e9-4389-be70-dfd1c1b4b253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neighborhood Size vs Price \n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(df[\"sqft_living15\"], df[\"price\"], alpha=0.3)\n",
    "plt.xlabel(\"Neighborhood Living Area (sqft_living15)\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"Neighborhood Size vs Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1a374a-aca6-4855-bb2d-6dc9edac8853",
   "metadata": {},
   "source": [
    "# Observation:\n",
    "Properties in neighborhoods with larger average living areas tend to have higher prices, indicating neighborhood-level effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3683be7-f65b-43bd-85ab-2d07565b0ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(\n",
    "    df[\n",
    "        [\n",
    "            \"price\",\n",
    "            \"sqft_living\",\n",
    "            \"grade\",\n",
    "            \"bathrooms\",\n",
    "            \"sqft_living15\",\n",
    "            \"view\",\n",
    "            \"waterfront\"\n",
    "        ]\n",
    "    ].corr(),\n",
    "    annot=True,\n",
    "    cmap=\"coolwarm\"\n",
    ")\n",
    "plt.title(\"Feature Correlation with Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331d8f82-c18a-414b-8a67-1b5f5dae6486",
   "metadata": {},
   "source": [
    "# Observation:\n",
    "Living area, grade, bathrooms, and neighborhood features show strong positive correlation with price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fea9db0-6ff7-47b2-959c-a96a6bb61ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Statistics\n",
    "df[\"price\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2834838c-4d5d-4e4d-9860-d2ac99b7c58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation with Target (Numeric Features Only)\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    numeric_df.corr(),\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0\n",
    ")\n",
    "plt.title(\"Correlation Heatmap (Numeric Features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae798990-18cf-46e7-b29b-e7ded35dbd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Satellite Images\n",
    "from PIL import Image\n",
    "\n",
    "sample_images = [f for f in os.listdir(IMAGE_PATH) if f.endswith(\".png\")][:4]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, img_name in enumerate(sample_images):\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    img = Image.open(os.path.join(IMAGE_PATH, img_name))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(img_name)\n",
    "\n",
    "plt.suptitle(\"Sample Satellite Images\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45332467-ec51-4eff-bd8c-94f31857caf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7256e9-d063-4d76-a337-f80cb5600bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f25aad9-08a2-459f-9340-2d12e9c64058",
   "metadata": {},
   "source": [
    "**SECTION 4 — Feature Engineering & Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3284a494-32ea-4531-bbaf-c23f108f7cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Target Transformation\n",
    "\n",
    "# Log-transform the target variable (justified by EDA)\n",
    "df[\"log_price\"] = np.log1p(df[\"price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f643bb58-f3d7-4770-bce6-630933b73738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Drop Unused Columns\n",
    "# Drop unused or non-predictive columns\n",
    "df = df.drop(columns=[\"id\", \"date\"], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7dd80a-9911-48df-a463-e901bf9c4ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.Feature Grouping\n",
    "# Numerical features\n",
    "num_features = [\n",
    "    \"bedrooms\", \"bathrooms\", \"sqft_living\", \"sqft_lot\",\n",
    "    \"sqft_above\", \"sqft_basement\", \"sqft_living15\",\n",
    "    \"sqft_lot15\", \"condition\", \"grade\", \"view\",\n",
    "    \"waterfront\", \"floors\", \"yr_built\", \"yr_renovated\"\n",
    "]\n",
    "\n",
    "# Categorical feature\n",
    "cat_features = [\"zipcode\"]\n",
    "\n",
    "# Geographical features (used for satellite imagery alignment)\n",
    "geo_features = [\"lat\", \"long\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4c1ac3-88bc-4960-810b-54e8f7cb8dce",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "\n",
    "Numerical features capture property size, structure, and quality\n",
    "\n",
    "Zipcode is treated as a categorical variable\n",
    "\n",
    "Latitude and longitude are kept separate for satellite image association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b3d9a3-0808-46be-8f97-45cba4881b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.Define Model Inputs and Target\n",
    "# Features used for tabular modeling\n",
    "X_tabular = df[num_features + cat_features]\n",
    "\n",
    "# Target variable\n",
    "y = df[\"log_price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5409af0e-2861-472d-85b3-30daa5269f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.Preprocessing Pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda8bc71-1def-4d2c-b395-e7299ee96f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical Pipeline\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c745e57d-8673-4876-898a-ebf7436cb4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.Combine Preprocessing Steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_features),\n",
    "        (\"cat\", categorical_transformer, cat_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f943ffd2-429e-4462-b012-84a68c3b50bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f501e0a-1427-4094-a54e-06a423d16cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93d125cf-c074-456d-92c2-a67870766ee8",
   "metadata": {},
   "source": [
    "**SECTION 5 — Train–Test Split**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74b5c2e-db6c-4dfb-88d1-4c14415d18fe",
   "metadata": {},
   "source": [
    "## Train–Test Split\n",
    "\n",
    "The dataset is split into training and testing sets to evaluate model\n",
    "performance on unseen data. The same split is used consistently across\n",
    "all models to ensure a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf82357-867b-4119-8510-e018231e7c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Perform the Train–Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tabular,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"Training samples:\", X_train.shape[0])\n",
    "print(\"Testing samples:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8274513-d3d3-4cdf-b091-e0fc55302d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Preserve Indices for Image Alignment, This is critical for your multimodal model.\n",
    "# Preserve indices for aligning satellite images later\n",
    "train_indices = X_train.index\n",
    "test_indices = X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b76f7-544b-4519-8119-b710883e1eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726d6ba0-fa40-4ed9-aa6c-43e9c6a9d933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6642993c-b24e-429d-b334-b3bf9a88531d",
   "metadata": {},
   "source": [
    "SECTION 6 — Baseline Model (Tabular Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9e0f52-470a-4fc9-a17e-fd9a413647fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Baseline Model (Tabular Only)\n",
    "\n",
    "A baseline model is trained using only tabular features to establish a\n",
    "reference level of performance. This allows us to quantify the improvement\n",
    "gained by more advanced models such as XGBoost and the multimodal approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387bc711-a294-4a65-94a9-63cf7443ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Build the Baseline Pipeline\n",
    "# We’ll use a simple linear model (Ridge Regression) as the baseline.\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac54e7b-479b-43ed-83f7-1ff10a3d530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", Ridge(alpha=1.0))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aba931-5402-422e-9172-940d6d0d4426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Train the Baseline Model\n",
    "baseline_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d95e7f8-54b8-441f-8aad-fa2b78aed9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.Evaluate the Baseline Model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "y_pred_baseline = baseline_model.predict(X_test)\n",
    "\n",
    "rmse_baseline = np.sqrt(mean_squared_error(y_test, y_pred_baseline))\n",
    "r2_baseline = r2_score(y_test, y_pred_baseline)\n",
    "\n",
    "print(\"Baseline Model Performance:\")\n",
    "print(\"RMSE:\", rmse_baseline)\n",
    "print(\"R²:\", r2_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87461d21-2c00-4cbf-b9ef-3ef87764ddb2",
   "metadata": {},
   "source": [
    "#Interpretation\n",
    "**Observation:**\n",
    "\n",
    "The baseline model provides a simple benchmark using linear relationships\n",
    "between features and price. While it captures general trends, it is limited\n",
    "in modeling non-linear interactions and complex feature relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c9a01b-9a65-4178-958b-5a3fe1f6b141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c57954-d335-4b0a-822c-644c6506ab1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6e72deb-9052-478d-93cd-64fb04cc991e",
   "metadata": {},
   "source": [
    "**SECTION 7 — XGBoost Model (Tabular Only)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ccded2-b6ce-493e-883c-e46f26bcf9c5",
   "metadata": {},
   "source": [
    "## XGBoost Model (Tabular Only)\n",
    "\n",
    "XGBoost is a powerful tree-based ensemble model that can capture non-linear\n",
    "relationships and feature interactions. In this section, we train an XGBoost\n",
    "model using only tabular features and compare its performance with the\n",
    "baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7023887e-5d7e-49b9-b6ec-08ae4c52bb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Import XGBoost\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724d1931-8f06-4965-85a0-ad152ced0979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Define the XGBoost Pipeline\n",
    "# We reuse the same preprocessing to ensure a fair comparison.\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", xgb_model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623bea2f-3559-4c0c-8bcd-5d652d10883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.Train the XGBoost Model\n",
    "xgb_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3952d3f6-15d2-42fe-b7e8-cfc28137a421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.Evaluate the XGBoost Model\n",
    "# Predictions\n",
    "y_pred_xgb = xgb_pipeline.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(\"XGBoost Model Performance:\")\n",
    "print(\"RMSE:\", rmse_xgb)\n",
    "print(\"R²:\", r2_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbce66b-4da5-44c1-bdbc-e95d4f587a1a",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "Interestingly, the baseline linear model slightly outperforms the XGBoost\n",
    "model on this particular train–test split. This suggests that much of the\n",
    "predictive signal in the tabular data can be captured through approximately\n",
    "linear relationships, especially after log-transforming the target variable.\n",
    "\n",
    "Given the strong performance of the baseline, the marginal difference between\n",
    "models is small and may vary with different data splits or further\n",
    "hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1238cc45-ccef-4902-9d41-35dd8e95f7f9",
   "metadata": {},
   "source": [
    "#Hyperparameter Tuning (XGBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c5fc27-3ba7-41b7-9e2f-d29a3ba3654c",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning (XGBoost)\n",
    "\n",
    "To further evaluate the potential of XGBoost, we perform hyperparameter tuning\n",
    "using cross-validation. This helps assess whether improved performance can be\n",
    "achieved beyond the default configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d944e9-a421-4db2-a4f2-be99e7827664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Parameter Grid\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a29cd-ca21-4ceb-ad1c-711e2ca53ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"model__n_estimators\": [200, 300, 500],\n",
    "    \"model__max_depth\": [3, 5, 7],\n",
    "    \"model__learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"model__subsample\": [0.7, 0.8, 1.0],\n",
    "    \"model__colsample_bytree\": [0.7, 0.8, 1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1930e2fa-d9fa-406c-b599-7c602a2006b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Randomized Search\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    estimator=xgb_pipeline,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20,\n",
    "    scoring=\"r2\",\n",
    "    cv=5,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "xgb_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c745a3a6-c0bb-47fc-a05b-45c1a769c990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Model Evaluation\n",
    "best_xgb = xgb_search.best_estimator_\n",
    "\n",
    "y_pred_xgb_tuned = best_xgb.predict(X_test)\n",
    "\n",
    "rmse_xgb_tuned = np.sqrt(mean_squared_error(y_test, y_pred_xgb_tuned))\n",
    "r2_xgb_tuned = r2_score(y_test, y_pred_xgb_tuned)\n",
    "\n",
    "print(\"Tuned XGBoost Performance:\")\n",
    "print(\"RMSE:\", rmse_xgb_tuned)\n",
    "print(\"R²:\", r2_xgb_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124787c0-baf6-48a2-b897-637e985c30d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Parameters\n",
    "xgb_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfc1558-490d-44cd-bea4-ce3a323d63ea",
   "metadata": {},
   "source": [
    "Comparison Summary\n",
    "Model\t                                  RMSE\t               R²\n",
    "Baseline (Ridge)\t                      0.1830\t           0.8786\n",
    "Default XGBoost                        \t  0.1895\t           0.8698\n",
    "Tuned XGBoost\t                          0.1783\t           0.8848"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b085a4-f3f6-4bfc-a04a-69dd084ae6f7",
   "metadata": {},
   "source": [
    "**Final Tabular Model Selection:**\n",
    "\n",
    "After hyperparameter tuning, XGBoost achieves the best performance among all\n",
    "tabular models. The improvement over both the baseline and the default XGBoost\n",
    "configuration highlights the importance of model optimization for capturing\n",
    "complex feature interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c6cc1b-d669-48fb-a209-19e754c5e317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdea7db-62dc-49d6-aada-756fbb02b1f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "709c6e75-f8e7-442c-8208-da36d5d104c0",
   "metadata": {},
   "source": [
    "**SECTION 8 — Multimodal Model (Tabular + Satellite Images)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f35adcc-6b40-416b-8c61-7a7d9aa4fd85",
   "metadata": {},
   "source": [
    "## Multimodal Model (Tabular + Satellite Images)\n",
    "\n",
    "In this section, we build a multimodal model that combines tabular property\n",
    "features with visual features extracted from satellite images. A convolutional\n",
    "neural network (CNN) is used to encode satellite images, and the resulting image\n",
    "embeddings are fused with tabular features to predict property prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd080085-c0b0-48a6-b4d5-b0d798dd70e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fixed random sample for multimodal learning\n",
    "# df_sample = df.sample(n=500, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "# print(\"Multimodal sample shape:\", df_sample.shape)\n",
    "print(\"Multimodal sample shape:\", df_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601eb562-9560-4e1c-85a1-6214ceff97c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Prepare Multimodal Dataset. We use the same 500-sample dataset you used for images.\n",
    "# Use the sampled dataset used for satellite images\n",
    "df_mm = df_sample.copy()\n",
    "\n",
    "# Target (log-price)\n",
    "y_mm = np.log1p(df_mm[\"price\"])\n",
    "\n",
    "# Tabular features\n",
    "X_tabular_mm = df_mm[num_features + cat_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829b4c4f-8ddf-407f-9919-a003c1877339",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_sample.index[:5])\n",
    "print(os.listdir(IMAGE_PATH)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5303b51-9982-44e6-a7ac-951d8e59a33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Train–Test Split (Multimodal) which is separate split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_tab_train, X_tab_test, y_mm_train, y_mm_test = train_test_split(\n",
    "    X_tabular_mm,\n",
    "    y_mm,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6308e245-1662-44bb-a656-b31b93e5d63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preserve indices for image loading:\n",
    "train_idx = X_tab_train.index\n",
    "test_idx = X_tab_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6718acbf-57e7-4c1e-b0d7-21ee21722bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.Preprocess Tabular Data (Reuse Pipeline)\n",
    "# Fit preprocessing on multimodal training data\n",
    "X_tab_train_proc = preprocessor.fit_transform(X_tab_train)\n",
    "X_tab_test_proc = preprocessor.transform(X_tab_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ecbed5-d0aa-4acf-b118-71afeb06ece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.Load Satellite Images\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "def load_images(indices):\n",
    "    images = []\n",
    "    for idx in indices:\n",
    "        img_path = os.path.join(IMAGE_PATH, f\"{idx}.png\")\n",
    "        if os.path.exists(img_path):  # safety check\n",
    "            img = load_img(img_path, target_size=IMG_SIZE)\n",
    "            img = img_to_array(img) / 255.0\n",
    "            images.append(img)\n",
    "    return np.array(images)\n",
    "X_img_train = load_images(train_idx)\n",
    "X_img_test = load_images(test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c00d480-086a-4b93-ab29-4933af4adc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.Build CNN for Image Feature Extraction\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ddff0-8c30-4312-bb2b-b7b994806aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn():\n",
    "    base_model = tf.keras.applications.ResNet50(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        input_shape=(224, 224, 3)\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(128, activation=\"relu\")\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "cnn_model = build_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d94074-2882-4a1c-ac71-5c0f9621ae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.Build Multimodal Fusion Model\n",
    "# Tabular branch\n",
    "tabular_input = layers.Input(shape=(X_tab_train_proc.shape[1],))\n",
    "tabular_dense = layers.Dense(64, activation=\"relu\")(tabular_input)\n",
    "\n",
    "# Image branch\n",
    "image_input = layers.Input(shape=(224, 224, 3))\n",
    "image_features = cnn_model(image_input)\n",
    "\n",
    "# Fusion\n",
    "combined = layers.concatenate([tabular_dense, image_features])\n",
    "combined_dense = layers.Dense(64, activation=\"relu\")(combined)\n",
    "output = layers.Dense(1)(combined_dense)\n",
    "\n",
    "multimodal_model = models.Model(\n",
    "    inputs=[tabular_input, image_input],\n",
    "    outputs=output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4242fd76-508f-4154-a846-85d4694d197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.Compile & Train the Model\n",
    "multimodal_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mse\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1ee716-b816-40f1-bf1d-32485e2ac441",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = multimodal_model.fit(\n",
    "    [X_tab_train_proc, X_img_train],\n",
    "    y_mm_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=10,\n",
    "    batch_size=16,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d238d0e-a52b-493e-a5eb-4dd95d5b628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.Evaluate Multimodal Model\n",
    "y_mm_pred = multimodal_model.predict([X_tab_test_proc, X_img_test]).flatten()\n",
    "\n",
    "rmse_mm = np.sqrt(mean_squared_error(y_mm_test, y_mm_pred))\n",
    "r2_mm = r2_score(y_mm_test, y_mm_pred)\n",
    "\n",
    "print(\"Multimodal Model Performance:\")\n",
    "print(\"RMSE:\", rmse_mm)\n",
    "print(\"R²:\", r2_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f84cfec-a5c2-4c08-b0e0-665d43bd4fed",
   "metadata": {},
   "source": [
    "**Multimodal Model Analysis:**\n",
    "\n",
    "The multimodal model demonstrates lower predictive performance compared to the\n",
    "tabular-only models. This outcome can be attributed to the limited size of the\n",
    "image dataset and the already strong predictive power of tabular features such\n",
    "as property size, quality, and amenities.\n",
    "\n",
    "While satellite imagery provides valuable contextual information about the\n",
    "surrounding environment, effectively leveraging visual features typically\n",
    "requires a substantially larger image dataset or task-specific fine-tuning.\n",
    "Despite the lower quantitative performance, the multimodal approach highlights\n",
    "the potential of integrating visual context for real estate valuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b67d3-738a-4b83-ba0a-8ad60c8b6543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cb1b1bc-e3d3-45fc-90d6-d57a891fb70c",
   "metadata": {},
   "source": [
    "### Visual Explainability using Grad-CAM\n",
    "\n",
    "To interpret how satellite images influence the multimodal model’s predictions,\n",
    "Grad-CAM visualizations were generated for selected test images. Grad-CAM\n",
    "highlights the spatial regions of an image that contribute most strongly to the\n",
    "model’s output, providing insight into which visual patterns the CNN focuses on\n",
    "when estimating property prices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2331a711-5987-45ee-8922-02b6dd803d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setups\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from PIL import Image\n",
    "\n",
    "IMG_SIZE = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d9d2d6-db2a-4f0e-85aa-31db87a87b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the ResNet50 backbone from the trained CNN\n",
    "resnet_model = cnn_for_gradcam.get_layer(\"resnet50\")\n",
    "\n",
    "# Sanity check: should show ResNet50 layers\n",
    "resnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c0e50e-face-4f7c-9ac9-cd8b76cef2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.Grad-CAM Function\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name):\n",
    "    grad_model = Model(\n",
    "        inputs=model.input,\n",
    "        outputs=[\n",
    "            model.get_layer(last_conv_layer_name).output,\n",
    "            model.output\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_array)\n",
    "        loss = predictions[:, 0]\n",
    "\n",
    "    grads = tape.gradient(loss, conv_outputs)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    conv_outputs = conv_outputs[0]\n",
    "    heatmap = tf.reduce_sum(conv_outputs * pooled_grads, axis=-1)\n",
    "\n",
    "    heatmap = tf.maximum(heatmap, 0)\n",
    "    heatmap /= tf.reduce_max(heatmap)\n",
    "    return heatmap.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85692f15-303d-48e6-a973-073b72872d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.Image Loader\n",
    "def load_image(img_path):\n",
    "    img = load_img(img_path, target_size=IMG_SIZE)\n",
    "    img = img_to_array(img) / 255.0\n",
    "    return np.expand_dims(img, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52edb04e-b327-4ef9-ace4-d51bc162ce8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gradcam_heatmap(img_array, resnet_model, last_conv_layer_name):\n",
    "    grad_model = Model(\n",
    "        inputs=resnet_model.input,\n",
    "        outputs=[\n",
    "            resnet_model.get_layer(last_conv_layer_name).output,\n",
    "            resnet_model.output\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_array)\n",
    "        loss = tf.reduce_mean(predictions)\n",
    "\n",
    "    grads = tape.gradient(loss, conv_outputs)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    conv_outputs = conv_outputs[0]\n",
    "    heatmap = tf.reduce_sum(conv_outputs * pooled_grads, axis=-1)\n",
    "\n",
    "    heatmap = tf.maximum(heatmap, 0)\n",
    "    heatmap /= tf.reduce_max(heatmap)\n",
    "    return heatmap.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77b997e-6b70-4926-99c9-d27d9fabf965",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a810b82-06f9-4889-8336-334c439fa9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRADCAM_DIR = \"gradcam_outputs\"\n",
    "os.makedirs(GRADCAM_DIR, exist_ok=True)\n",
    "\n",
    "image_ids = [10, 50, 100]  # 2–3 images is enough\n",
    "\n",
    "for i in image_ids:\n",
    "    img_path = f\"images/{i}.png\"\n",
    "    img_array = load_image(img_path)\n",
    "\n",
    "    heatmap = make_gradcam_heatmap(\n",
    "        img_array,\n",
    "        resnet_model,\n",
    "        last_conv_layer_name=\"conv5_block3_out\"\n",
    "    )\n",
    "\n",
    "    # Load original image\n",
    "    img = Image.open(img_path).resize(IMG_SIZE)\n",
    "    img = np.array(img)\n",
    "\n",
    "    # Resize heatmap\n",
    "    heatmap_resized = Image.fromarray(np.uint8(255 * heatmap)).resize(IMG_SIZE)\n",
    "    heatmap_resized = np.array(heatmap_resized)\n",
    "\n",
    "    # Apply colormap\n",
    "    heatmap_color = plt.cm.jet(heatmap_resized)[:, :, :3] * 255\n",
    "\n",
    "    # Overlay heatmap\n",
    "    overlay = (0.6 * img + 0.4 * heatmap_color).astype(np.uint8)\n",
    "\n",
    "    # Show\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(overlay)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Grad-CAM (Image {i})\")\n",
    "    plt.show()\n",
    "\n",
    "    # Save\n",
    "    plt.imsave(f\"{GRADCAM_DIR}/gradcam_{i}.png\", overlay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fb91f9-8f72-4f89-9aaa-e5eb00957813",
   "metadata": {},
   "source": [
    "**Grad-CAM Interpretation:**\n",
    "\n",
    "The Grad-CAM heatmaps indicate that the CNN focuses on regions containing dense\n",
    "building structures, road networks, and surrounding green spaces. These visual\n",
    "patterns capture neighborhood-level characteristics such as urban density and\n",
    "environmental quality, which are known to influence property prices. This\n",
    "provides interpretability for how satellite imagery contributes to the\n",
    "multimodal model’s predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fc5799-198e-4a3d-8a16-60fcdf6e9484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023f5c27-d4d5-4691-a95e-05408eb65019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc187543-9343-4884-8d36-848ccc090f5c",
   "metadata": {},
   "source": [
    "** SECTION 9 — Model Evaluation & Comparison**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4fc051-a912-4e21-96a6-a0d5e055963f",
   "metadata": {},
   "source": [
    "## Model Evaluation & Comparison\n",
    "\n",
    "In this section, we compare the performance of all trained models using the same\n",
    "evaluation metrics. This comparison highlights the strengths and limitations of\n",
    "each modeling approach, including tabular-only and multimodal models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d42d50e-37a6-45ee-8430-83d2aefeba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\n",
    "        \"Baseline (Ridge)\",\n",
    "        \"XGBoost (Default)\",\n",
    "        \"XGBoost (Tuned)\",\n",
    "        \"Multimodal (Tabular + Satellite Images)\"\n",
    "    ],\n",
    "    \"RMSE (log-price)\": [\n",
    "        0.183009208706201,\n",
    "        0.18952830241383828,\n",
    "        0.17832588139033603,\n",
    "        0.36939097447658004\n",
    "    ],\n",
    "    \"R²\": [\n",
    "        0.8786304675516019,\n",
    "        0.8698296903829609,\n",
    "        0.8847628388586452,\n",
    "        0.47765999855354047\n",
    "    ]\n",
    "})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52e5b05-66be-48f1-87e3-b636bdf81967",
   "metadata": {},
   "source": [
    "**Results Interpretation:**\n",
    "\n",
    "The tuned XGBoost model achieves the best overall performance among all models,\n",
    "demonstrating the importance of modeling non-linear relationships and applying\n",
    "hyperparameter optimization for tabular data.\n",
    "\n",
    "The baseline Ridge regression model performs competitively, indicating that a\n",
    "large portion of the predictive signal can be captured through approximately\n",
    "linear relationships after log-transforming the target variable.\n",
    "\n",
    "The multimodal model, which integrates satellite imagery with tabular features,\n",
    "exhibits lower predictive performance. This outcome is primarily due to the\n",
    "limited size of the satellite image dataset and the strong predictive power of\n",
    "the tabular features. Despite lower quantitative performance, the multimodal\n",
    "approach provides valuable visual interpretability through Grad-CAM and\n",
    "demonstrates the potential of incorporating spatial context into property price\n",
    "prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729d6803-2e23-4b99-92b8-51b0854f5991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3f1a33-2a19-4fe0-8b06-5b2a62252a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65dd30e2-d46a-44a8-ab31-ce789aa15198",
   "metadata": {},
   "source": [
    "**SECTION 10 — Final Predictions (CSV)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b443fa-4dfd-409d-aac6-e5bd83912264",
   "metadata": {},
   "source": [
    "## Final Predictions\n",
    "\n",
    "Using the best-performing model (Tuned XGBoost), final property price\n",
    "predictions are generated and exported as a CSV file in the required format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e0a916-c9dd-4d01-abcc-4f49accbaeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "# Load test data (Excel)\n",
    "test_df = pd.read_excel(DATA_PATH + \"test2.xlsx\")\n",
    "\n",
    "print(\"Test data shape:\", test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7639fa96-6a7f-4b0b-8c18-ab6a30b8ed4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store IDs for submission\n",
    "test_ids = test_df[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8336b804-03f9-4497-876f-8ada6d1b2fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unused columns\n",
    "test_df_model = test_df.drop(columns=[\"id\", \"date\"], errors=\"ignore\")\n",
    "\n",
    "# Select tabular features\n",
    "X_test_final = test_df_model[num_features + cat_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137abd5a-3c4e-470d-8fef-5f6e767f863c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check duplicate IDs\n",
    "dup_counts = test_df[\"id\"].value_counts()\n",
    "dup_ids = dup_counts[dup_counts > 1]\n",
    "\n",
    "print(\"Number of duplicated IDs:\", len(dup_ids))\n",
    "dup_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3ed815-4768-485f-b213-a02bb374b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate IDs, keep first occurrence\n",
    "test_df_unique = test_df.drop_duplicates(subset=\"id\", keep=\"first\")\n",
    "\n",
    "print(\"Original test size:\", test_df.shape)\n",
    "print(\"After removing duplicates:\", test_df_unique.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8376d4c-a276-479d-9868-b87d54c4a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OUTPUT_PATH = \"outputs\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e020ad9-8570-4e9d-b6e4-a06fcfde56e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preserve IDs\n",
    "test_ids = test_df_unique[\"id\"]\n",
    "\n",
    "# Drop unused columns\n",
    "test_df_model = test_df_unique.drop(columns=[\"id\", \"date\"], errors=\"ignore\")\n",
    "\n",
    "# Select features\n",
    "X_test_final = test_df_model[num_features + cat_features]\n",
    "\n",
    "# Predict log-price\n",
    "test_log_preds = best_xgb.predict(X_test_final)\n",
    "\n",
    "# Convert to original scale\n",
    "test_price_preds = np.expm1(test_log_preds)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test_ids,\n",
    "    \"predicted_price\": test_price_preds\n",
    "})\n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae9b8d3-d58f-4a0c-a8a7-5ad90b591338",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(os.path.join(OUTPUT_PATH, \"22323041_Final.csv\"), index=False)\n",
    "\n",
    "print(\"Prediction file saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158e3474-3a00-4c71-9679-acbaa2836527",
   "metadata": {},
   "source": [
    "## Duplicate property entries were identified in the test dataset. To ensure a\n",
    "single prediction per property, duplicate IDs were removed by retaining the\n",
    "first occurrence before generating final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f820e5a5-381e-405d-a59f-f83e6fee0e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dcdb02-c121-4036-a4af-1f7126e5c4f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d2c859-4e46-49a4-a800-3bc3eaa00a34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "188cebe1-e5cf-4c75-b2d8-d0f112f6ae13",
   "metadata": {},
   "source": [
    "**SECTION 11 — Conclusion & Future Work**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ce772d-d432-4d6a-88a1-6ae015204ed4",
   "metadata": {},
   "source": [
    "## Conclusion & Future Work\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "In this project, we developed and evaluated multiple models for satellite-based\n",
    "property price prediction, ranging from simple linear baselines to advanced\n",
    "tree-based and multimodal deep learning approaches.\n",
    "\n",
    "The baseline Ridge regression model provided a strong reference point,\n",
    "demonstrating that many relationships in the tabular data are approximately\n",
    "linear after log-transforming the target variable. A tuned XGBoost model\n",
    "achieved the best overall performance among tabular-only models, highlighting\n",
    "the importance of non-linear modeling and hyperparameter optimization for\n",
    "structured property data.\n",
    "\n",
    "A multimodal model integrating satellite imagery with tabular features was also\n",
    "explored. While the multimodal approach did not outperform tabular-only models\n",
    "in terms of predictive accuracy, it provided valuable visual interpretability\n",
    "through Grad-CAM and demonstrated the potential of incorporating spatial and\n",
    "environmental context into real estate valuation.\n",
    "\n",
    "---\n",
    "\n",
    "### Future Work\n",
    "\n",
    "Several directions could further improve the multimodal modeling approach:\n",
    "\n",
    "- Expanding the satellite image dataset to include a larger and more diverse\n",
    "  set of properties, which would allow the CNN to learn more robust visual\n",
    "  representations.\n",
    "- Fine-tuning the CNN backbone on real estate–specific imagery to better capture\n",
    "  features relevant to property valuation.\n",
    "- Incorporating additional spatial data sources, such as points of interest,\n",
    "  zoning information, or street-level imagery.\n",
    "- Applying more advanced multimodal fusion strategies and attention mechanisms\n",
    "  to better balance the contributions of tabular and visual features.\n",
    "\n",
    "Overall, this project demonstrates the feasibility of combining structured data\n",
    "with satellite imagery for property price prediction while highlighting both\n",
    "the strengths and current limitations of multimodal learning in real-world\n",
    "settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b3a1c1-76e9-4992-828b-95adfcf7e44e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54477725-256a-4c5d-8671-8c2530f55833",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
